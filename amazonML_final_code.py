# -*- coding: utf-8 -*-
"""AmazonDataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J3etyCTBlIu_394PuXMzZQLVvBHJQ3FN

# Tabular Feature EDA
"""

import pandas as pd
df = pd.read_csv("catalog_features_reduced_more.csv")
# Basic stats
print(df['price'].describe())
print("zeros or negatives:", (df['price']<=0).sum())
print("duplicates:", df.duplicated(subset=['sample_id']).sum())
# Histogram and skew
import matplotlib.pyplot as plt
plt.hist(df['price'].dropna(), bins=100)
plt.title('price distribution (raw)')
plt.show()

"""# Data Cleaning"""

import numpy as np
# skew check
print("skew:", df['price'].skew())
# trim extreme
upper = df['price'].quantile(0.999)  # 99.9th percentile
df = df[df['price'] <= upper]
# log transform target
df['log_price'] = np.log1p(df['price'])
df

df.info()

df.shape

# Text imputation + missing flags
df['description'] = df['description'].fillna('')
df['desc_missing'] = (df['description'] == '').astype(int)

# brand
df['brand'] = df['brand'].fillna('missing_brand')
df['brand_missing'] = (df['brand'] == 'missing_brand').astype(int)

# Fill missing numeric columns with their mean
numeric_cols = df.select_dtypes(include=np.number).columns
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())


# compute price_per_unit if possible
mask = df['price_per_unit'].isna() & df['pack_count'].notna() & df['price'].notna()
df.loc[mask, 'price_per_unit'] = df.loc[mask, 'price'] / df.loc[mask, 'pack_count']

# numeric impute (using median after potential calculation)
for col in ['pack_count','weight_g','price_per_unit']:
    df[col] = df[col].fillna(df[col].median())

df.info()

"""Dependencies download"""

# Cell 0 - imports and helpers
import os
import time
import numpy as np
import pandas as pd
from pathlib import Path
from io import BytesIO
from PIL import Image
from tqdm import tqdm
import requests
import traceback

# For combining sparse & dense
from scipy import sparse

# Metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Utility
def save_np(path, arr):
    np.save(path, arr)

def load_np(path):
    return np.load(path)

def filename_from_url(url):
    try:
        return Path(url).name
    except:
        return ""

df.shape

"""# Image Feature Extraction using ResNet"""

# Cell 2A - ResNet50 URL extraction (save to .npy for reuse)
# Install tensorflow if needed. This cell uses Keras ResNet50 as feature extractor.
try:
    from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
    from tensorflow.keras.preprocessing import image
    import tensorflow as tf
    print("TensorFlow version", tf.__version__)
    USE_RESNET = True
except Exception as e:
    print("TensorFlow/ResNet not available.", e)
    USE_RESNET = False

def extract_resnet_feature_from_pil(img_pil, model):
    img = img_pil.resize((224,224))
    x = np.array(img, dtype=np.float32)
    if x.ndim == 2:  # grayscale
        x = np.stack([x]*3, axis=-1)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    feat = model.predict(x, verbose=0)
    return feat.reshape(-1)

def extract_resnet_from_url_list(urls, batch_save_path="resnet_features.npy", batch_size=128, cache=True):
    if cache and os.path.exists(batch_save_path):
        print("Loading cached features from", batch_save_path)
        return load_np(batch_save_path)
    if not USE_RESNET:
        raise RuntimeError("TensorFlow & ResNet not available.")
    model = ResNet50(weights='imagenet', include_top=False, pooling='avg')
    features = []
    for u in tqdm(urls):
        try:
            r = requests.get(u, timeout=4)
            r.raise_for_status()
            img = Image.open(BytesIO(r.content)).convert('RGB')
            feat = extract_resnet_feature_from_pil(img, model)
        except Exception:
            # on failure return zeros (same shape as model output)
            feat = np.zeros((model.output_shape[1],), dtype=np.float32)
        features.append(feat)
    features = np.vstack(features)
    save_np(batch_save_path, features)
    return features

# Cell 3 - select which extractor to run. You can run both if you want to compare.
image_urls = df['image_link'].fillna('').astype(str).tolist()

# Option A: ResNet
if USE_RESNET:
    resnet_feats = extract_resnet_from_url_list(image_urls, batch_save_path="resnet_features.npy", cache=True)
    print("resnet_feats shape:", resnet_feats.shape)

# If neither available, create a placeholder zero-array so later code runs
if not (USE_RESNET or USE_CLIP):
    print("Neither image extractor available; creating zero placeholders.")
    resnet_feats = np.zeros((len(df), 2048), dtype=np.float32)
    clip_feats = np.zeros((len(df), 512), dtype=np.float32)

"""## Tabular + text preprocessing"""

# Cell 4 - Preprocessing & engineered features
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# 1) Basic imputations and flags
df['description'] = df['description'].fillna('')
df['desc_missing'] = (df['description'].str.strip() == '').astype(int)

# brand: if high-cardinality, do frequency encoding; else do one-hot
df['brand'] = df['brand'].fillna('missing_brand')
brand_counts = df['brand'].value_counts(normalize=True)
# frequency encoding
df['brand_freq'] = df['brand'].map(brand_counts).fillna(0.0)

# price_per_unit: compute where possible
mask = df['price_per_unit'].isna() & df['pack_count'].notna() & (df['pack_count']>0)
df.loc[mask, 'price_per_unit'] = df.loc[mask, 'price'] / df.loc[mask, 'pack_count']

# numeric columns
numeric_cols = ['pack_count', 'weight_g', 'price_per_unit', 'desc_word_count']  # add other numeric cols present
# create desc_word_count
df['desc_word_count'] = df['description'].str.split().apply(len)

# fill numeric median
for c in numeric_cols:
    if c in df.columns:
        df[c] = df[c].fillna(df[c].median())

# binary/textual flags
df['has_image'] = df['image_link'].notna() & (df['image_link'] != '')
df['has_image'] = df['has_image'].astype(int)

# TEXT vectorization for item_name (high signal)
text_col = 'item_name'
df[text_col] = df[text_col].fillna('')
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), min_df=3)
X_text = tfidf.fit_transform(df[text_col].values.astype('U'))
print("TF-IDF shape:", X_text.shape)

# Prepare numeric & categorical arrays
num_arr = df[numeric_cols].values if all(c in df.columns for c in numeric_cols) else np.zeros((len(df),0))
# Standardize numeric
scaler = StandardScaler()
if num_arr.shape[1]>0:
    num_arr = scaler.fit_transform(num_arr)

# categorical small-card columns example -> one-hot
cat_small = []
for c in ['has_barcode','has_ingredients']:  # change as per your df
    if c in df.columns:
        cat_small.append(c)
if cat_small:
    cat_df = pd.get_dummies(df[cat_small].fillna(0).astype(int), drop_first=False)
    cat_arr = cat_df.values
else:
    cat_arr = np.zeros((len(df),0))

# brand frequency column already numeric: brand_freq
brand_arr = df[['brand_freq']].values

# combine numeric + cat + brand_freq + has_image + desc_missing etc
extra_cols = []
for c in ['has_image','desc_missing']:
    if c in df.columns:
        extra_cols.append(df[[c]].values)
if extra_cols:
    extra_mat = np.hstack(extra_cols)
else:
    extra_mat = np.zeros((len(df),0))

# Final dense features
dense_X = np.hstack([num_arr, brand_arr, cat_arr, extra_mat]) if (num_arr.size or brand_arr.size or cat_arr.size or extra_mat.size) else np.zeros((len(df),0))

# Convert dense to sparse and hstack with text
if dense_X.size==0:
    X_tab_sparse = sparse.csr_matrix(np.zeros((len(df),1)))
else:
    X_tab_sparse = sparse.csr_matrix(dense_X)
X_combined_tab_text = sparse.hstack([X_tab_sparse, X_text], format='csr')
print("Combined tab+text sparse shape:", X_combined_tab_text.shape)

"""# Merge image features with tabular/text"""

df.columns

# If both exist, you can concatenate them. Example uses ResNet if available, else CLIP.
use_resnet = 'resnet_feats' in globals()
use_clip = 'clip_feats' in globals()

if use_resnet and 'resnet_feats' in globals():
    img_feats = resnet_feats
elif use_clip and 'clip_feats' in globals():
    img_feats = clip_feats
else:
    img_feats = np.zeros((len(df), 1))

print("img_feats shape:", img_feats.shape)

# Convert image feats to sparse for hstack (or keep dense and vstack later)
img_sparse = sparse.csr_matrix(img_feats)

# Final full matrix: tab+text + image
X_full = sparse.hstack([X_combined_tab_text, img_sparse], format='csr')
print("Final X_full shape:", X_full.shape)

# Target (log)
y = df['log_price'].values

X_tab_dense = X_combined_tab_text.toarray()   # dense shape: (n_samples, tab_text_dim)
print("Tabular+text dense shape:", X_tab_dense.shape)

y

"""# Model training â€” Train-Test Split

---


"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_full, y, test_size=0.2, random_state=42
)

X_full

y.shape

"""# Training the model-XGBoost Regressor"""

import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Initialize model
model = xgb.XGBRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# Train
model.fit(X_train, y_train)

# Predict
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"MSE: {mse:.3f}, R2: {r2:.3f}")

y.shape

# Predict
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

y_train.shape

y_train

y_pred_train

y_pred_train.shape

y_test.shape

y_test

y_pred_test

y_pred_test.shape

"""# actual"""

import numpy as np

y_pred_before = np.concatenate([y_train, y_test])
y_pred_before

y_pred_before_withoutlog = np.exp(y_pred_before)  # log1p inverse
y_pred_before_withoutlog

"""# predictions"""

import numpy as np

y_pred = np.concatenate([y_pred_train, y_pred_test])
y_pred.shape

import numpy as np
y_pred_withoutlog = np.exp(y_pred)  # log1p inverse

y_pred_withoutlog

y_pred_withoutlog.shape

submission = pd.DataFrame({
    "ID": df["sample_id"],  # ya koi unique identifier column agar hai
    "Predicted_Price": y_pred_withoutlog
})

submission.to_csv("predictions.csv", index=False)
print("âœ… Predictions saved!")

"""# Deployment of Model"""

import joblib

# Save the trained model
joblib.dump(model, "xgb_model.pkl")

model = joblib.load("xgb_model.pkl")